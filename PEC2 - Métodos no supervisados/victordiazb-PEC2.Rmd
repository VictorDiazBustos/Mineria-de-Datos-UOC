---
title: 'Minería de datos: PEC2 - Métodos no supervisados'
author: "Autor: Víctor Díaz Bustos"
date: "Noviembre 2022"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: 75.584-PEC-header.html
  word_document: default
  pdf_document:
    highlight: zenburn
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

------------------------------------------------------------------------

## Ejercicios

Los ejercicios se realizarán en base al juego de datos *Hawks* presente en el paquete R *Stat2Data*.

Los estudiantes y el profesorado del Cornell College en Mount Vernon, Iowa, recogieron datos durante muchos años en el mirador de halcones del lago MacBride, cerca de Iowa City, en el estado de Iowa. El conjunto de datos que analizamos aquí es un subconjunto del conjunto de datos original, utilizando sólo aquellas especies para las que había más de 10 observaciones. Los datos se recogieron en muestras aleatorias de tres especies diferentes de halcones: Colirrojo, Gavilán y Halcón de Cooper.

Hemos seleccionado este juego de datos por su parecido con el juego de datos *penguins* y por su potencial a la hora de aplicarle algoritmos de minería de datos no supervisados. Las variables numéricas en las que os basaréis son: *Wing*, *Weight*, *Culmen*, *Hallux*

```{r message= FALSE, warning=FALSE}
if (!require('Stat2Data')) install.packages('Stat2Data')
library(Stat2Data)
data("Hawks")
summary(Hawks)
```

## Ejercicio 1

Presenta el juego de datos, nombre y significado de cada columna, así como las distribuciones de sus valores.

Adicionalmente realiza un estudio tipo EDA (exploratory data analysis) similar al de los ejemplos 1.1 y 1.2 ( *k-means* )

### Respuesta 1

Para este estudio se va a analizar el juego de datos *Hawks* presente en el paquete R *Stat2Data*, el cual contiene información de halcones recigoda durante muchos años en el mirador de halcones del lago MacBride, y en el que han filtrado aquellas especies con, al menos, 10 observaciones.

Dicho juego de datos cuenta con las variables '***Month***', '***Day***', '***Year***', '***CaptureTime***' y '**ReleaseTime**' para indicar el momento en el que se realizó cada registro; '***BandNumber***' como identificador del halcón; '***Species***' para indicar la especie de halcón (CH, SS ó RT); '***Age***' para indicar la edad del halcón (I - Infant ó A - Adult); '***Sex***' género del halcón (F - Femenino ó M - masculino); '***Wing***' longitud de las alas (mm); '***Weight***' peso del halcón (gr); '***Culmen***' altura del halcón (cm); '***Hallux***' longitud del hallux (mm); '***Tail***' longitud de la cola (mm); '***StandardTail***' longitud de cola estándar (mm); '***Tarsus***' longitud del torso (cm); '***WingPitFat***' grosor de la fosa de las alas (0, 1, 2 ó 3); '***KeelFat***' grasa de la quilla; '***Crop***' cosecha.

De dicho juego de datos, utilizaremos únicamente las variables '*Wing'*,'*Weight'*, '*Culmen'* y'*Hallux'* para nuestro análisis.

```{r message=FALSE, warning=FALSE}
# Datos con los que voy a trabajar, etiquetados
n = c('Wing','Weight','Culmen','Hallux','Species')
labeled_data <- Hawks[n]
str(labeled_data)
```

En primer lugar, para el análisis preliminar de las variables, vamos a necesitar trabajar con datos en la misma magnitud en la medida de lo posible, por lo que se convertirán las variables '*Culmen'* de cm a mm para tenerlas en la misma magnitud que'*Wing'* y'*Hallux'.* Además, se eliminarán aquellas filas que contengan algún valor nulo. Esto último podemos hacerlo a que las variables que estamos estudiando contienen un número reducido de valores nulos. En caso contrario, habría que plantearse eliminar alguna columna.

```{r message=FALSE, warning=FALSE}
# Eliminar filas con algún valor nulo
labeled_data <- na.omit(labeled_data)

# Multiplico x10 para pasar de cm a mm
labeled_data$Culmen <- labeled_data$Culmen * 10

# Muestro la nueva estructura del juego de datos
str(labeled_data)
```

A continuación, vamos a ver como se distribuyen los datos de cada variable. Para ello, vamos a visualizarlos mediante gráficos de barras:

```{r message=FALSE, warning=FALSE}
# Añadir librerías
if(!require('ggplot2')) install.packages('ggplot2'); library('ggplot2')
if(!require('dplyr')) install.packages('dplyr'); library('dplyr')

# Datos con los que voy a trabajar, sin etiquetar
n = c('Wing','Weight','Culmen','Hallux')
data <- labeled_data[n]

# Resumen estadístico del juego de datos
summary(data)

# Mostrar una gráfica para cada columna
for (i in colnames(data)){
  ggp <- ggplot(data, aes_string(x = i)) +
    geom_histogram(bins = 30, fill = "cornflowerblue", color = "black",ggtittle ="Contador ocurrencias") 
  plot(ggp)
}
```

Observaciones:

-   *Wing*: Comprende registros entre 37.2 mm y 480 mm de longitud de ala. Tiene una mayor concentración de registros en torno a los valores 200 y 400, con una media aritmética de 315.6 mm, aunque dicho valor promedio no es un valor muy común, ya que, el valor promedio de los valores de especies distintas puede dar un valor que no represente a ninguna de las especies presentes.

-   *Weight*: Comprende registros entre 56 gr y 2230 gr de peso. Tiene una mayor concentración de registros en torno a los valores 100 y 1000, con valor promedio de 772.1 gr, que, al igual que en la variable '*Wing*', no es un valor común en la muestra.

-   *Culmen*: Comprende registros entre 86 mm y 392 mm de altura. Tiene una mayor concentración de registros en torno a los valores 120 y 260, con valor promedio de 218 mm, que, al igual que en los casos anteriores, no es un valor común en la muestra.

-   *Hallux*: Comprende registros entre 9.5 mm y 341.4 mm de altura. Tiene una granconcentración de registros en torno a los valores iniciales (el 75% de los registros están compendidos entre 9.5 y 31.4 mm), mientras que se ven valores muy dispersos por encima, que apuntan a que podrían ser outliers. En este caso el valor promedio es 29.4, el cual sí es un valor representativo de la muestra. Además, dado que se aprecia una única clara concentración de valores, parece que el peso no depende de la especie, y que estas tienen un peso similar. Para comprobar esto, vamos a hacer un "zoom" a la gráfica:

```{r message=FALSE, warning=FALSE}
# Zoom al la gráfica de 'Hallux' con límites en 0 y 50 en el eje x
ggp <- ggplot(data, aes_string(x = 'Hallux')) +
    geom_histogram(bins = 30, fill = "cornflowerblue", color = "black",ggtittle ="Contador ocurrencias") +
  scale_x_continuous(limits = c(0, 50))
  plot(ggp)
```

Ahora sí que se pueden apreciar dos concentraciones de valores, pudiendo intuir que las distintas especies sí que tienden a un valor u otro.

A continuación, vamos a utilizar el algoritmo *k-means* para agrupar los datos.

El algoritmo ***k-means*** es un algoritmo de clasificación no supervisada que requiere que de antemano se fijen los k grupos que quieren obtenerse.

Supongamos un juego de datos *X = {x~1~, x~2~, ..., x~n~}* donde cada *x~i~* podría ser una observación con *m* atributos *x~i~ = {x~i1~, x~i2~ , ..., x~im~}.* Para clasificar nuestro juego de datos X, el algoritmo k-means sigue los siguientes pasos:

1.  De entre las *n* observaciones selecciona *k*, al que llamaremos semillas, y denotaremos por *c~j~* donde *j* *= 1,\...,k*. Cada semilla *c~j~* identificará su clúster *C~j~*.
2.  Asigna la observación *x~i~* al clúster *C~t~* cuando la distancia entre la observación *x~i~* y la semilla *c~t~* sea la menor entre todas las semillas. *d(x~i,~c~t~ ) = min{d(x~i,~c~j~ )}, j = 1, ..., k.*
3.  Calcula los nuevos centroides a partir de las medias de los clústeres actuales.
4.  Como criterio de parada, calcula la mejora que se produciría si asignáramos una observación a un clúster al que no pertenece actualmente. Entendiendo por mejora, por ejemplo, la minimización de la distancia de las distintas observaciones a sus respectivos centros
5.  Realiza el cambio que mayor mejora proporciona.
6.  Repite los pasos 3, 4 y 5 hasta que ningún cambio sea capaz de proporcionar una mejora significativa.

Para implementarlo, en primer lugar utilizamos la función *daisy()* para calcular todas las diferencias por pares (distancias) entre observaciones en el conjunto de datos. Posteriormente, aplicamos k-medias con distintos valores para *k* y calculamos el valor promedio de la silueta en cada caso, entendiendo la silueta como la evaluación de cómo de bien o mal está clasificada la muestra en el clúster al que ha sido asignada. Para ello se usa una fórmula que tiene en cuenta la distancia a las muestras de su clúster y la distancia a las muestras del clúster vecino más cercano.

```{r message= FALSE, warning=FALSE}
if (!require('cluster')) install.packages('cluster')
library(cluster)

# Calculo disimilaridad de los datos
d <- daisy(data)

# Calculo la silueta para cada valor de k
resultados <- rep(0, 10)
for (i in c(2,3,4,5,6,7,8,9,10))
{
  fit           <- kmeans(data, i)
  y_cluster     <- fit$cluster
  sk            <- silhouette(y_cluster, d)
  resultados[i] <- mean(sk[,3])
}
```

Mostramos en un gráfica los valores de las siluetas media de cada prueba para comprobar que número de clústers es el mejor.

```{r message= FALSE, warning=FALSE}
# Gráfica con la evolución del valor promedio de la silueta en función de k
plot(2:10,resultados[2:10],type="o",col="blue",pch=0,xlab="Número de clusters",ylab="Silueta")
```

Contrariamente a lo que esperaríamos al saber de antemano que el juego de datos contiene 3 especies, observamos como la mejora más significativa se obtiene para valores de k igual a 2. De este modo llegaríamos a la conclusión de que las variables seleccionadas para tratar de describir las 3 especies son capaces de identificar 2 grupos y no 3.

Otra forma de evaluar cual es el mejor número de clústers es considerar el mejor modelo, aquel que ofrece la menor suma de los cuadrados de las distancias de los puntos de cada grupo con respecto a su centro (withinss), con la mayor separación entre centros de grupos (betweenss). Como se puede comprobar es una idea conceptualmente similar a la silueta. Una manera común de hacer la selección del número de clústers consiste en aplicar el método *elbow* (codo), que no es más que la selección del número de clústers en base a la inspección de la gráfica que se obtiene al iterar con el mismo conjunto de datos para distintos valores del número de clústers. Se seleccionará el valor que se encuentra en el "codo" de la curva.

```{r message= FALSE, warning=FALSE}
# Calculo la suma de los cuadrados de las distancias de los puntos de cada
# grupo respecto de su centro para cada valor de k
resultados <- rep(0, 10)
for (i in c(2,3,4,5,6,7,8,9,10))
{
  fit           <- kmeans(data, i)
  resultados[i] <- fit$tot.withinss
}
plot(2:10,resultados[2:10],type="o",col="blue",pch=0,xlab="Número de clusters",ylab="tot.tot.withinss")
```

En este caso el número óptimo de clústers son 4 que es cuando la curva comienza a estabilizarse.

También se puede usar la función *kmeansruns* del paquete **fpc** que ejecuta el algoritmo kmeans con un conjunto de valores, para después seleccionar el valor del número de clústers que mejor funcione de acuerdo a dos criterios: la silueta media ("asw") y *Calinski-Harabasz* ("ch").

```{r message= FALSE, warning=FALSE}
if (!require('fpc')) install.packages('fpc')
library(fpc)
# Criterio de la silueta media para cada valor de k
fit_ch  <- kmeansruns(data, krange = 1:10, criterion = "ch") 

# Criterio de Calinski-Harabasz para cada valor de k
fit_asw <- kmeansruns(data, krange = 1:10, criterion = "asw") 
```

Podemos comprobar el valor con el que se ha obtenido el mejor resultado.

```{r message= FALSE, warning=FALSE}
# Muestro resultados
fit_ch$bestk
fit_asw$bestk
```

Merece la pena pararse en este punto.\
Observemos como la primera recomendación es de 10 clústers mientras que la segunda es de 2.\
Obviamente 10 clústers es una anomalía y nos tiene que hacer ver que estos métodos no pueden tomarse al pie de la letra. Conviene contrastarlos con diferentes aproximaciones y pasarlos por el filtro del sentido común, es decir, debemos tratar de dar respuesta a la siguiente pregunta. Conociendo el caso de estudio y los datos, ¿tiene sentido este resultado?.

Una buena estrategia es ver que pasa para diferentes valores de k utilizando los dos criterios.

```{r message= FALSE, warning=FALSE}
# Muestro la evolución del valor de la silueta media en función de k
plot(1:10,fit_ch$crit,type="o",col="blue",pch=0,xlab="Número de clústers",ylab="Criterio Calinski-Harabasz")

# Muestro la evolución del valor de Calinski-Harabasz en función de k
plot(1:10,fit_asw$crit,type="o",col="blue",pch=0,xlab="Número de clústers",ylab="Criterio silueta media")
```

Los resultados son muy parecidos a los que hemos obtenido anteriormente. Con el criterio de la silueta media se obtienen dos clústers y con el *Calinski-Harabasz* se obtienen 3.

Como se ha comprobado, conocer el número óptimo de clústers no es un problema fácil. Tampoco lo es la evaluación de los modelos de agregación.

Como en el caso que estudiamos sabemos que los datos pueden ser agrupados en 3 clases o especies, vamos a ver cómo se ha comportado *kmeans* en el caso de pedirle 3 clústers. Para eso comparamos visualmente los campos dos a dos, con el valor real que sabemos está almacenado en el campo "species" del dataset original.

(Aclaramos que obviamente no acostumbra a pasar que conozcamos de forma previa el número de clústers óptimo. Este ejemplo lo planteamos con finalidades didácticas y con voluntad de experimentar)

```{r message= FALSE, warning=FALSE}
# Aplico k-means para k=3
hawks3clusters <- kmeans(data, 3)

# Muestro clasificación obtenida por k-means
plot(data, col=hawks3clusters$cluster, main="Clasificación k-means")

# Muestro clasificación real
plot(data, col=as.factor(labeled_data$Species), main="Clasificación real")

# Muestro la distribución real de cada especie para saber a cual pertenece cada color
n = c("Wing","Weight","Culmen","Hallux")
CH <- subset(labeled_data, Species=="CH")
plot(CH[n], col=as.factor(CH$Species), main="CH")

RT <- subset(labeled_data, Species=="RT")
plot(RT[n], col=as.factor(RT$Species), main="RT")

SS <- subset(labeled_data, Species=="SS")
plot(SS[n], col=as.factor(SS$Species), main="SS")
```

Clasificación real:

-   Negro: "CH"

-   Rojo: "RT"

-   Verde: "SS"

El grupo formado por puntos negros contiene tanto la especie '*CH'* como'*SS*', mientras que la especie '*RT*' está repartida entre los grupos de color verde y rojo.

Una buena técnica que ayuda a entender los grupos que se han formado, es mirar de darles un nombre. Cómo por ejemplo:

-   Grupo 1: '*CH' y*'*SS*' (color negro)
-   Grupo 2: '*RT*' (color rojo y verde)

## Ejercicio 2

Con el juego de datos proporcionado realiza un estudio similar al del ejemplo 2 ( *DBSCAN i OPTICS* )

### Respuesta 2

En este ejemplo vamos ha trabajar los algoritmos **DBSCAN** y **OPTICS** como métodos de clustering que permiten la generación de grupos no radiales a diferencia de k-means. Es decir, el algoritmo k-means estructuralmente está construido alrededor del concepto de distancia de cada observación a su centroide asignado, este diseño provoca que k-means tenga tendencia a generar grupos con forma esférica colocando el centroide en el centro.

Veremos que su parámetro de entrada más relevante es *minPts* que define la mínima densidad aceptada alrededor de un centroide. Incrementar este parámetro nos permitirá reducir el ruido (observaciones no asignadas a ningún cluster).

El algoritmo **DBSCAN** (*Density-based Spatial Clustering of Applications with Noise*) requiere que se le informe de dos parámetros:

-   El valor ε (**epsilon**): máximo radio de vecindad. Consideraremos que dos puntos u observaciones están cercanos si la distancia que los separa es menor o igual a ε.

-   El valor **minPts**: mínimo número de puntos en la ε-vecindad de un punto. Podemos pensarlo como el valor que marcará nuestro criterio de qué consideramos como denso.

De este modo, DBSCAN irá construyendo esferas de radio ε que al menos incluyan minPts observaciones. La lógica que sigue el algoritmo para construir los clústeres o zonas densamente pobladas es la siguiente:

-   Se considera que un punto *p* es un **punto núcleo**, *core point*, si al menos tiene minPts puntos a una distancia menor o igual a ε. Dicho de otro modo, contiene *minPts* en la ε-vecindad.

-   Un punto *q* es **alcanzable** desde *p*, (*p-reachable*), donde *p* es núcleo, si la distancia entre ambos es inferior o igual a ε. Dicho de otro modo, si está dentro de la ε-vecindad de *p*.

-   Un punto *q* es alcanzable desde *p*, si existe un camino de puntos núcleo que los conecta. Explicado más formalmente, si existe *p~1~,\...,p~n~*, con *p~1~ = p* y *p~n~ = q*, donde cada *p~i+1~* es alcanzable por *p~i~* y todos los *p~1~,\...,p~n-1~* son puntos núcleo.

-   Cualquier punto no alcanzable se considerará punto extremo o outlier.

La siguiente figura nos muestra de un modo esquemático, el proceso de construcción de zonas de densidad. En este ejemplo se toma minPts = 4.

![](images/paste-454CCF0F.png)

Los puntos B y C corresponden a la frontera del clúster, es decir, son puntos alcanzables desde un punto núcleo, pero ellos mismo no son punto núcleo porque no incluyen minPts en su ε-vecindario. Los puntos A son puntos núcleo ya que como mínimo cada uno de ellos tiene 4 puntos en un radio ε pre-fijado. Finalmente, el punto N se considera extremo o outlier puesto que no es alcanzable desde ningún punto del juego de datos.

Por su parte, el algoritmo **OPTICS** (*Ordering Points to Identify Cluster Structure*), es un algoritmo que de algún modo generaliza DBSCAN y resuelve su principal inconveniente: los parámetros iniciales.

OPTICS requiere un radio ε y un criterio de densidad *minPts* igual que DBSCAN, pero en el caso de OPTICS el valor de radio ε no determinará la formación de clústeres sino que servirá para ayudar a reducir la complejidad de cálculo en el propio algoritmo. En realidad OPTICS no es un algoritmo que genere una propuesta de clústeres a partir de un juego de datos de entrada, como DBSCAN. De hecho, lo que hace es ordenar los puntos del juego de datos en función de su distancia de alcanzabilidad, o ***reachability distance***, en inglés. Para entender bien este concepto nuevo, nos basaremos en la siguiente figura, donde hemos tomado *minPts* = 5.

![](images/paste-EB433B1C.png)

La ***core-distance*** del punto p es el radio ε' mínimo tal que su ε′-vecindad contiene al menos minPts = 5 puntos. La ***reachability-distance*** de un punto q respecto de un punto núcleo (corepoint) p será la mayor de las dos distancias siguientes:

-   *core-distance* del punto p,

-   distancia euclidiana entre los puntos p y q, que denotaremos por *d(p,q).*

Siguiendo con el ejemplo de la figura anterior, vemos cómo la *reachability-distance* de los puntos *p* y *q~1~* es la *core-distance* del punto *p*, porque esta es mayor que la distancia euclidiana entre los puntos *p* y *q*. Por otro lado, la *reachability-distance* de los puntos p y q~2~ es la distancia euclidiana entre ellos, porque esta es mayor que la *core-distance* del punto p. OPTICS como algoritmo lo que nos va a hacer es asignar a cada punto del juego de datos una *reachability-distance*. Aclarados estos conceptos básicos, podemos avanzar en la comprensión de la utilidad de disponer de dicha ordenación. Para ello usaremos un tipo de gráfico específico para este algoritmo, el *reachability plot*. Para entender bien qué es un *reachability plot* veamos la siguiente figura. En el gráfico inferior vemos la *reachability-distance* asignada a cada punto y apreciamos como hay zonas con valores altos que se corresponden con los puntos *outliers* y zonas con valores muy bajos que se corresponden con puntos ubicados en zonas densas.

![](images/paste-7D7F3E49.png)

Fijémonos que a la hora de generar los clústeres podremos decidir cuál es la *reachability-distance* límite que nos marca qué consideramos como clúster. Podremos calibrar o ajustar este valor límite hasta conseguir una generación de clústeres adecuada. La posibilidad de calibrar la *reachability-distance* límite, hace que OPTICS en realidad lo que nos dé es una ordenación de puntos por *reachability-distance* y en consecuencia será el propio analista quien podrá generar múltiples combinaciones de clústeres en función del límite que se quiera fijar. Una de las primeras actividades que realiza el algoritmo es **ordenar las observaciones** de forma que los puntos más cercanos se conviertan en vecinos en el ordenamiento. Se podría pensar como una representación numérica del dendograma de una agrupación jerárquica.

```{r message= FALSE, warning=FALSE}
# Añadimos la librería 'dbscan'
if (!require('dbscan')) install.packages('dbscan')
library(dbscan)

### Lanzamos el algoritmo OPTICS dejando el parámetro eps con su valor por defecto y fijando el criterio de vecindad en 10
res <- optics(data, minPts = 10)
res
```

```{r message= FALSE, warning=FALSE}
### Obtenemos la ordenación de las observaciones o puntos
res$order
```

Otro paso muy interesante del algoritmo es la generación de un **diagrama de alcanzabilidad** o *reachability plot,* en el que se aprecia de una forma visual la distancia de alcanzabilidad de cada punto.

Los valles representan clusters (cuanto más profundo es el valle, más denso es el cluster), mientras que las cimas indican los puntos que están entre las agrupaciones (estos puntos son cadidatos a ser considerados *outliers*)

```{r message= FALSE, warning=FALSE}
### Gráfica de alcanzabilidad
plot(res)
```

Veamos otra representación del diagrama de alcanzabilidad, donde podemos observar las trazas de las distancias entre puntos cercanos del mismo cluster y entre clusters distintos.

```{r message= FALSE, warning=FALSE}
### Dibujo de las trazas que relacionan puntos
# Wing y Weight
plot(data[c(1,2)], col = "grey")
polygon(data[c(1,2)][res$order,])

# Wing y Culmen
plot(data[c(1,3)], col = "grey")
polygon(data[c(1,3)][res$order,])

# Wing y Hallux
plot(data[c(1,4)], col = "grey")
polygon(data[c(1,4)][res$order,])

# Weight y Culmen
plot(data[c(2,3)], col = "grey")
polygon(data[c(2,3)][res$order,])

# Weight y Hallux
plot(data[c(2,4)], col = "grey")
polygon(data[c(2,4)][res$order,])

# Culmen y Hallux
plot(data[c(3,4)], col = "grey")
polygon(data[c(3,4)][res$order,])
```

Otro ejercicio interesante a realizar es extraer una agrupación de la ordenación realizada por OPTICS similar a lo que DBSCAN hubiera generado estableciendo el parámetro eps en eps_cl = 0.065. En este sentido animamos al estudiante a experimentar con diferentes valores de este parámetro.

```{r message= FALSE, warning=FALSE}
### Extracción de un clustering DBSCAN cortando la alcanzabilidad en el valor eps_cl
res <- extractDBSCAN(res, eps_cl = .065)
res
plot(res) ## negro indica ruido
```

En este caso, se ve la gráfica de alcanzabilidad completamente negra. Esto se debe a que el valor de eps_cl no es lo suficientemente grande y por tanto no se alcanzan minPts vecinos dentro del radio definido por eps_cl, definiendo todos los puntos como ruido.

```{r message= FALSE, warning=FALSE}
hullplot(data, res)
```

Como se puede apreciar, no se ha detectado ningún cluster.

Repetimos el experimento anterior aumentando el valor de eps_cl, aumentando así el radio en el que se acepte un vecino, permitiendo que sea más factible alcanzar mintPts vecinos.

```{r message= FALSE, warning=FALSE}
### Incrementamos el parámetro eps
res <- extractDBSCAN(res, eps_cl = 10)
res
plot(res)
hullplot(data, res)
```

En este caso ya se logran generar algunos clústeres, aunque sigue habiendo mucho ruido debido a que eps_cl sigue sin ser lo suficientemente grande.

A continuación, calculamos el valor promedio de la silueta para cuantificar la calidad del agrupamiento.

```{r message= FALSE, warning=FALSE}
# Calculo disimilaridad de los datos
d <- daisy(data)

sk <- silhouette(res$cluster, d)
print(mean(sk[,3]))
```

Como cabía esperar, obtenemos un valor muy bajo. Esto se debe a que, a pesar de que los puntos de un clúster están muy cercanos entre sí, hay muchos puntos clasificados como ruido, pero que siguen estando muy cerca de algún clúster, teniendo puntos en el clúster vecino muy cercanos, y reduciendo así la calidad del agrupamiento.

Por tanto volvemos a repetir con un valor de eps_cl aún mayor.

```{r message= FALSE, warning=FALSE}
### Incrementamos el parámetro eps
res <- extractDBSCAN(res, eps_cl = 30)
res
plot(res)
hullplot(data, res)
```

Observamos en los gráficos anteriores como se han coloreado los 5 clusters y en negro se mantienen los valores *outliers* o extremos.

A continuación, comparamos las agrupaciones obtenidas con la clasificación real.

```{r message= FALSE, warning=FALSE}
# Muestro nube de puntos de clasificación real
plot(labeled_data[,1:4], col=as.factor(labeled_data$Species), main="Clasificación real")

# Muestro nube de puntos de clasificación OPTICS
plot(data, col=res$cluster, main="Clasificación OPTICS")
```

Como se puede apreciar, el clúster de puntos **negros** contiene los datos de la especie '*RT*'. Para poder analizar bien el resto de clústers, vamos a hacer un zoom a las gráficas.

```{r message= FALSE, warning=FALSE}
# Muestro 6 gráficas en 1
layout(matrix(c(1:6), nrow=2, byrow=TRUE))

# Wing y Weight REAL
plot(labeled_data[c(1,2)], col=as.factor(labeled_data$Species), main="Clasificación real", xlim = c(100,300))
# Weight y Culmen REAL
plot(labeled_data[c(2,3)], col=as.factor(labeled_data$Species), main="Clasificación real", xlim = c(0,500))
# Culmen y Hallux REAL
plot(labeled_data[c(3,4)], col=as.factor(labeled_data$Species), main="Clasificación real", xlim = c(50,200))

# Wing y Weight OPTICS
plot(data[c(1,2)], col=res$cluster, main="Clasificación OPTICS", xlim = c(100,300))
# Weight y Culmen OPTICS
plot(data[c(2,3)], col=res$cluster, main="Clasificación OPTICS", xlim = c(0,500))
# Culmen y Hallux OPTICS
plot(data[c(3,4)], col=res$cluster, main="Clasificación OPTICS", xlim = c(50,200))


```

Ahora podemos apreciar que los clusters de puntos **rojos** y **verdes** coinciden con los datos de la especie "*CH*" y los clusters de puntos **cian** y **azul** coinciden con los datos de la especie "*SS*". Además, hay algunos puntos que desaparecen ya que estos son catalogados como *outliers* y no pertenecen a ningún clúster.

Una buena técnica que ayuda a entender los grupos que se han formado, es mirar de darles un nombre. Cómo por ejemplo:

-   Grupo 1: '*RT*' (color negro)
-   Grupo 2: '*SS*' (color cian y azul)
-   Grupo 3: '*CH*' (color rojo y verde)

A continuación, calculamos el valor promedio de la silueta para cuantificar la calidad del agrupamiento.

```{r message= FALSE, warning=FALSE}
sk <- silhouette(res$cluster, d)
print(mean(sk[,3]))
```

En este caso, la calidad es mayor al del caso anterior, ya que se consigue mantener una corta distancia entre puntos de un mismo clúster, reduciendo en gran medida la cantidad de ruido en el agrupamiento.

Repetimos el experimento incrementando más aún el valor de eps_cl . Veamos como el efecto que produce es la concentración de clusters ya que flexibilizamos la condición de densidad.

```{r message= FALSE, warning=FALSE}
### Incrementamos el parámetro eps
res <- extractDBSCAN(res, eps_cl = 100)
res
plot(res)
hullplot(data, res)
```

```{r message= FALSE, warning=FALSE}
plot(data, col=as.factor(labeled_data$Species), main="Clasificación real")
plot(data, col=res$cluster, main="Clasificación OPTICS")
```

En este caso, obtenemos 2 únicos clústers, el de puntos de color negro que contiene los datos de la especie "*RT*" y el de puntos de color rojos que contiene datos tanto de la especie "*SS*" como "*CH*". Además, el número de *outliers* disminuye en este caso. Así, podemos definir los siguientes grupos:

-   Grupo 1: '*CH' y*'*SS*' (color rojo)
-   Grupo 2: '*RT*' (color negro)

A continuación, calculamos el valor promedio de la silueta para cuantificar la calidad del agrupamiento.

```{r message= FALSE, warning=FALSE}
sk <- silhouette(res$cluster, d)
print(mean(sk[,3]))
```

En este caso la calidad vuelve a disminuir. Esto es debido a que se forman clústers muy amplios, donde la distancia entre puntos de un mismo clúster aumenta, haciendo que disminuya la calidad del agrupamiento.

A continuación vemos como evoluciona la calidad del modelo para distintos valores de eps_cl, así de cuantos clústeres se obtienen en cada caso para ver qué valor de eps_cl es el mejor para un determinado número de clusteres.

```{r message= FALSE, warning=FALSE}
# Calculo disimilaridad de los datos
d <- daisy(data)

# Calculo la silueta para cada valor de k
resultados <- rep(1, 11)
num_clusters <- rep(1, 11)
i = 1
n = c(10,20,40,60,80,100,120,140,160,180,200)
for (eps in n)
{
  res <- extractDBSCAN(res, eps_cl = eps)
  sk <- silhouette(res$cluster, d)
  resultados[i] <- mean(sk[,3])
  num_clusters[i] <- max(res$cluster)
  i = i+1
}
# Gráfica con la evolución del valor promedio de la silueta en función de eps_cl
plot(n ,resultados,type="o",col="blue",pch=0,xlab="eps_cl",ylab="Silueta")

# Gráfica con la evolución del número de clústeres en funcion de eps_cl
plot(n ,num_clusters,type="o",col="blue",pch=0,xlab="eps_cl",ylab="Silueta")

```

Como se puede apreciar, a medida que aumenta el valor de eps_cl, aumenta la calidad del agrupamiento, ya que de esta manera se reduce el ruido. Sin embargo, hay un momento en el que el valor de eps_cl es tan grande que se genera un único clúster, haciendo que la calidad el clúster caiga.

Para este caso que conocemos el número de clústers a generar (3), el mejor eps_cl probado es **80**. A continuación muestro su agrupación.

```{r message= FALSE, warning=FALSE}
res <- extractDBSCAN(res, eps_cl = 80)
res
plot(res)
hullplot(data, res)
```

## Ejercicio 3

Aprovechando el hecho de que a priori conocemos el grupo correcto al que pertenece cada observación del juego de datos, realiza una comparativa de los métodos *k-means* y *DBSCAN* en base a los resultados obtenidos en los ejercicios 1 y 2, contextualizando en el caso de estudio.

### Respuesta 3

Partiendo del hecho de que conocemos el grupo correcto al que pertenece cada observación del juego de datos, y, por tanto, el número de grupos existentes (3 en este caso), comparamos las soluciones de k-means y DBSCAN que han generado 3 clústers, midiendo la calidad de agrupamiento por el valor promedio de la silueta. En el caso de DBSCAN, elegiré *eps_cl=80*, ya que ha sido el valor de épsilon probado que genera 3 clústeres que mejor valoración nos ha dado.

```{r message= FALSE, warning=FALSE}
# Calculo disimilaridad de los datos
d <- daisy(data)

# Calculo el valor promedio de la silueta de k-means para k=3
fit_kmeans <- kmeans(data, 3)
silueta_kmeans <- silhouette(fit_kmeans$cluster, d)
valoracion_kmeans <- mean(silueta_kmeans[,3])
valoracion_kmeans

# Calculo el valor promedio de la silueta de DBSCAN para eps_cl=80
fit_dbscan <- extractDBSCAN(res, eps_cl = 80)
silueta_dbscan <- silhouette(fit_dbscan$cluster, d)
valoracion_dbscan <- mean(silueta_dbscan[,3])
valoracion_dbscan
```

En este caso, se obtiene un mejor agrupamiento con DBSCAN. Aunque esto realmente tiene trampa, porque se han probado previamente varios valores de eps_cl para ver cual generaba un mejor agrupamiento con 3 clústeres. Si aplicasemos el algoritmo a ciegas en ese sentido, lo más probable es que realizase un agrupamiente poco óptimo para un determinado valor de k, mientras que k-means siempre maximiza en la medida de lo posible la calidad de agrupamiento para un determinado k.

Así, se pueden observar algunas de las ventajas y desventajas de estos algoritmos:

-   k-means es ideal si se conoce el número de grupos existentes en el juego de datos, mientras que DBSCAN no logra sacar provecho a priori de esa información, requiere de varias pruebas para obtener un buen valor para minPts y épsilon.

-   Cuando se realiza un buen ajuste de parámetros, DBSCAN obtiene mejores resultados que k-means. Esto es gracias a que DBSCAN está menos limitado que k-means a las formas geométricas de los grupos y se ve menos influenciado por los outliers

En conclusión, siempre que se conozca el valor de K, el algoritmo k-medias partirá con ventaja sobre DBSCAN. Sin embargo, este no siempre será el más adecuado, ya que se ve muy limitado por la distribución de los datos y por los outliers. Por su parte, DBSCAN tiene un mayor potencial, siendo mucho más robusto a outliers y formas geométricas, aunque requiere de un estudio más profundo para poder ajustar sus parámetros.

## Ejercicio 4

Analiza ventajas y desventajas de cada uno de los dos métodos estudiados en esta práctica ( *k-means* y *DBSCAN* ).\
Adicionalmente realiza una propuesta de medidas para tratar de mitigar las desventajas identificadas.

### Respuesta 4

**Ventajas k-means**

-   Escala a **grandes conjuntos de datos.**

-   Garantiza la **convergencia**.

-   Fácil de **implementar**.

**Contras k-means**

-   Tener que **fijar como parámetro de entrada el valor de k**. Acertar con el valor óptimo del número de clústers requiere de experiencia y conocimiento tanto sobre el algoritmo en sí como sobre el propio juego de datos.

-   Sólo genera **clústeres de forma circular**.

-   Depende de los **valores iniciales de los centroides**.

-   Problemas para agrupar datos con **clústeres condiferentes tamaños y densidades**.

-   Los **centroides se pueden arrastrar por valores atípicos**, o bien los **valores atípicos pueden obtener su propio clúster** en lugar de ignorarlos.

**Ventajas DBSCAN**

-   Es capaz de identificar **clústeres de cualquier forma geométrica**, no solo circular, ya que solo necesita que exista la combinación de zonas con alta y baja densidad de concentración de puntos.

-   Es especialmente bueno **identificando valores extremos**. Para DBSCAN no supone ningún inconveniente trabajar con un juego de datos con este tipo de valores.

-   **No requiere que le prefijemos el número de clústeres** que queremos que identifique. Lo único que necesita es que haya zonas de baja densidad de puntos para así poder marcar bien las fronteras entre clústeres.

**Contras DBSCAN**

-   Tener que **fijar como parámetros de entrada los valores *ε* y *minPts***. Acertar con el valor óptimo de estos parámetros requiere de experiencia y conocimiento tanto sobre el algoritmo en sí como sobre el propio juego de datos.

-   No puede agrupar correctamente **conjuntos de datos con grandes diferencias en las densidades**, ya que la combinación de MinPts no se puede escoger adecuadamente para todos los grupos.

**Medidas para mitigar las desventajas**

K-MEANS

Para encontrar el número de clusters en los datos, deberemos ejecutar el algoritmo para un rango de valores K, ver los resultados y comparar características de los grupos obtenidos. En general no hay un modo exacto de determinar el valor real de K, pero se puede estimar con aceptable precisión siguiendo la esta técnica.

DBSCAN

Para elegir el número mínimo de puntos que componen un clúster (minPts), típicamente se debe tomar una valor mayor o igual a la dimensionalidad del conjunto de datos.

Un método para obtener valor de épsilon consiste en calcular las distancias vecinas más cercanas en una matriz de puntos. Consiste en calcular el promedio de las distancias de cada punto a sus vecinos más cercanos y determinar el punto en el que se produce un cambio brusco a lo largo de la curva de distancias.
